{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning GPT2\n",
    "* Named entity recognition task\n",
    "* N2C2 2006 & 2014 Deidentification Challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/nvme2/n2c2/'\n",
    "model_dir = '/media/nvme2/models/redact/'\n",
    "model_in_fname = 'ckpt.pt'\n",
    "device_type = 'cuda'\n",
    "device = 'cuda'\n",
    "block_size = 1024\n",
    "batch_size = 6\n",
    "eval_iters = 200\n",
    "eval_interval = 1000\n",
    "log_interval = 100\n",
    "max_iters = 10000\n",
    "weight_decay = 1e-2\n",
    "learning_rate = 6e-5\n",
    "beta1, beta2 = 0.9, 0.95\n",
    "warmup_iters = 500\n",
    "lr_decay_iters = 10000\n",
    "min_lr = 6e-6\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and define simple data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_data = np.load(os.path.join(data_dir, 'train.npy'))\n",
    "val_data = np.load(os.path.join(data_dir, 'val.npy'))\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size, 0]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i:i+block_size, 1]).astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('redact/data/n2c2/label2id.json', 'r') as f:\n",
    "    label2id = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_labels = max(label2id.values())\n",
    "n_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_layer': 12,\n",
       " 'n_head': 12,\n",
       " 'n_embd': 768,\n",
       " 'block_size': 1024,\n",
       " 'dropout': 0.0,\n",
       " 'vocab_size': 50304,\n",
       " 'bias': False,\n",
       " 'out_size': 50304}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(model_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "model_args = checkpoint['model_args']\n",
    "model_args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.37M\n"
     ]
    }
   ],
   "source": [
    "from redact.gpt2_model import GPTConfig, GPT\n",
    "\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace language model head  \n",
    "We need the model output features to match the number of NER labels (17).  Since the current head is tied to the token embedding layer, we'll simply add two additional linear layers that reduce our output from vocab_size to n_labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.replace_head(n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=50304, bias=False)\n",
       "    (1): Linear(in_features=50304, out_features=1572, bias=False)\n",
       "    (2): Linear(in_features=1572, out_features=786, bias=False)\n",
       "    (3): Linear(in_features=786, out_features=17, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze all layers except new head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head[0].parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay, learning_rate, (beta1, beta2), device_type\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_lr(it):\n",
    "    # During warmup iterations: linear\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # After decay iterations: minimum\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # Decay iterations\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            if loss.item() < 0:\n",
    "                print(f'Negative eval loss\\n\\tlogits: {logits}\\n\\ttarget: {Y}')\n",
    "            else:\n",
    "                losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 57.8851, val loss 58.1419\n",
      "Iter 0: loss 61.2802, time145445.54ms\n",
      "Iter 100: loss 6.9475, time419.86ms\n",
      "Iter 200: loss 2.0517, time428.57ms\n",
      "Iter 300: loss 6.6653, time428.03ms\n",
      "Iter 400: loss 18.2460, time422.32ms\n",
      "Iter 500: loss 5.4727, time418.16ms\n",
      "Iter 600: loss 41.0305, time419.49ms\n",
      "Iter 700: loss 3.2128, time422.63ms\n",
      "Iter 800: loss 5.4458, time426.02ms\n",
      "Iter 900: loss 3.8344, time423.53ms\n",
      "Step 1000: train loss 1.3502, val loss 1.3177\n",
      "Iter 1000: loss 0.3833, time129602.49ms\n",
      "Iter 1100: loss 3.0226, time423.52ms\n",
      "Iter 1200: loss 2.2068, time419.56ms\n",
      "Iter 1300: loss 1.2828, time426.56ms\n",
      "Iter 1400: loss 1.9225, time423.90ms\n",
      "Iter 1500: loss 3.9992, time425.93ms\n",
      "Iter 1600: loss 0.2880, time418.41ms\n",
      "Iter 1700: loss 1.0225, time422.67ms\n",
      "Iter 1800: loss 1.1405, time423.41ms\n",
      "Iter 1900: loss 0.1888, time421.22ms\n",
      "Step 2000: train loss 0.9422, val loss 0.9615\n",
      "Iter 2000: loss 0.4805, time129643.82ms\n",
      "Iter 2100: loss 0.3215, time419.07ms\n",
      "Iter 2200: loss 0.4267, time425.55ms\n",
      "Iter 2300: loss 1.1898, time428.16ms\n",
      "Iter 2400: loss 0.3265, time420.90ms\n",
      "Iter 2500: loss 0.5305, time422.82ms\n",
      "Iter 2600: loss 0.8054, time424.17ms\n",
      "Iter 2700: loss 0.5229, time419.30ms\n",
      "Iter 2800: loss 0.2122, time418.72ms\n",
      "Iter 2900: loss 0.0640, time421.58ms\n",
      "Step 3000: train loss 0.4543, val loss 0.4496\n",
      "Iter 3000: loss 0.4157, time129825.08ms\n",
      "Iter 3100: loss 0.2215, time429.54ms\n",
      "Iter 3200: loss 0.3008, time420.56ms\n",
      "Iter 3300: loss 0.3491, time422.14ms\n",
      "Iter 3400: loss 0.2489, time421.88ms\n",
      "Iter 3500: loss 0.1564, time428.71ms\n",
      "Iter 3600: loss 0.3116, time421.20ms\n",
      "Iter 3700: loss 0.3442, time428.61ms\n",
      "Iter 3800: loss 0.1833, time421.59ms\n",
      "Iter 3900: loss 0.0950, time426.67ms\n",
      "Step 4000: train loss 0.4702, val loss 0.4469\n",
      "Iter 4000: loss 0.0601, time129810.12ms\n",
      "Iter 4100: loss 0.1340, time428.23ms\n",
      "Iter 4200: loss 0.2714, time422.17ms\n",
      "Iter 4300: loss 0.2624, time420.79ms\n",
      "Iter 4400: loss 0.1523, time422.10ms\n",
      "Iter 4500: loss 0.3898, time430.49ms\n",
      "Iter 4600: loss 0.5075, time420.19ms\n",
      "Iter 4700: loss 0.2394, time424.89ms\n",
      "Iter 4800: loss 0.2028, time423.62ms\n",
      "Iter 4900: loss 0.2413, time422.80ms\n",
      "Step 5000: train loss 0.6002, val loss 0.5712\n",
      "Iter 5000: loss 0.2215, time129888.95ms\n",
      "Iter 5100: loss 0.1063, time419.99ms\n",
      "Iter 5200: loss 0.1663, time421.55ms\n",
      "Iter 5300: loss 0.2208, time426.73ms\n",
      "Iter 5400: loss 0.1383, time428.82ms\n",
      "Iter 5500: loss 0.1908, time421.13ms\n",
      "Iter 5600: loss 0.1506, time424.38ms\n",
      "Iter 5700: loss 0.1570, time422.98ms\n",
      "Iter 5800: loss 0.1718, time423.54ms\n",
      "Iter 5900: loss 0.2695, time423.90ms\n",
      "Step 6000: train loss 0.5284, val loss 0.5206\n",
      "Iter 6000: loss 0.1341, time129912.30ms\n",
      "Iter 6100: loss 0.2435, time423.88ms\n",
      "Iter 6200: loss 0.2510, time422.39ms\n",
      "Iter 6300: loss 0.4016, time424.62ms\n",
      "Iter 6400: loss 0.1120, time426.35ms\n",
      "Iter 6500: loss 0.0819, time424.56ms\n",
      "Iter 6600: loss 0.0801, time423.13ms\n",
      "Iter 6700: loss 0.2819, time423.95ms\n",
      "Iter 6800: loss 0.1172, time423.82ms\n",
      "Iter 6900: loss 0.2121, time425.21ms\n",
      "Step 7000: train loss 0.5567, val loss 0.5537\n",
      "Iter 7000: loss 0.0686, time130305.90ms\n",
      "Iter 7100: loss 0.1305, time422.26ms\n",
      "Iter 7200: loss 0.1260, time425.92ms\n",
      "Iter 7300: loss 0.2070, time420.95ms\n",
      "Iter 7400: loss 0.1905, time426.67ms\n",
      "Iter 7500: loss 0.0540, time425.44ms\n",
      "Iter 7600: loss 0.1369, time427.70ms\n",
      "Iter 7700: loss 0.1357, time424.15ms\n",
      "Iter 7800: loss 0.0772, time429.74ms\n",
      "Iter 7900: loss 0.1503, time427.12ms\n",
      "Step 8000: train loss 0.4765, val loss 0.4411\n",
      "Iter 8000: loss 0.0341, time130098.56ms\n",
      "Iter 8100: loss 0.0504, time421.90ms\n",
      "Iter 8200: loss 0.0985, time423.33ms\n",
      "Iter 8300: loss 0.1269, time424.05ms\n",
      "Iter 8400: loss 0.0911, time424.20ms\n",
      "Iter 8500: loss 0.0905, time426.61ms\n",
      "Iter 8600: loss 0.0786, time423.36ms\n",
      "Iter 8700: loss 0.1452, time428.54ms\n",
      "Iter 8800: loss 0.1030, time426.89ms\n",
      "Iter 8900: loss 0.0736, time427.29ms\n",
      "Step 9000: train loss 0.4958, val loss 0.4976\n",
      "Iter 9000: loss 0.0909, time129916.65ms\n",
      "Iter 9100: loss 0.1281, time419.99ms\n",
      "Iter 9200: loss 0.0730, time425.65ms\n",
      "Iter 9300: loss 0.0912, time424.67ms\n",
      "Iter 9400: loss 0.0596, time419.45ms\n",
      "Iter 9500: loss 0.1847, time422.70ms\n",
      "Iter 9600: loss 0.1451, time425.62ms\n",
      "Iter 9700: loss 0.0945, time426.09ms\n",
      "Iter 9800: loss 0.0987, time421.97ms\n",
      "Iter 9900: loss 0.1009, time422.93ms\n",
      "Step 10000: train loss 0.4624, val loss 0.4392\n",
      "Iter 10000: loss 0.1453, time129722.08ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Get learning rate\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Eval\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Get batch\n",
    "    X, Y = get_batch('train')\n",
    "\n",
    "    # Forward\n",
    "    _, loss = model(X, Y)\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Step optimizer\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Log\n",
    "    t1 = time.time()\n",
    "    dt = t1-t0\n",
    "    t0 = t1\n",
    "\n",
    "    if iter_num % log_interval == 0:\n",
    "        lossf = loss.item()\n",
    "        print(f'Iter {iter_num}: loss {lossf:.4f}, time{dt*1000:.2f}ms')\n",
    "\n",
    "    # Update iter_num and check for termination conditions\n",
    "    iter_num +=1\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
